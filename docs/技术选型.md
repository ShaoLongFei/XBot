# 基于开源项目的技术选型和任务分工

## 问题

我们是不是可以在原有的项目基础上做整合优化？如果我们以开源项目为基底的话，采用成熟的 AI 模型，我们该如何安排我们的任务？

## 回复

### 战略建议

是的，**强烈建议基于开源项目整合优化**，这是三天时间内最可行的方案。以下是具体建议：

---

### 一、技术选型建议

**推荐开源基座：**
- **Dify** - 适合快速搭建 LLM 应用，支持知识库、工作流编排
- **LangChain** - 灵活的 LLM 应用框架，适合定制化开发
- **FastChat** - 如果需要部署开源模型
- **Rasa** - 传统对话系统框架（备选）

**推荐 AI 模型：**
- **API 服务（推荐）**：GPT-4、Claude、文心一言、通义千问
- **开源模型（备选）**：ChatGLM3、Qwen、Baichuan2
- **向量模型**：text-embedding-ada-002 或 bge-large-zh

**选型原则：** 优先选择 API 服务，避免模型部署耗时

---

### 二、三人团队任务分工

#### 第一天（基础搭建）

**成员A - 数据工程师：**
- 清洗七牛客服数据（问答对提取、去重、分类）
- 构建向量数据库（使用 Pinecone/Milvus/Qdrant）
- 准备评估数据集（最近一个月问题）

**成员B - 后端开发：**
- 搭建开源框架基础环境（如 Dify/LangChain）
- 实现 RAG（检索增强生成）流程
- API 接口开发

**成员C - 产品/全栈：**
- 设计系统架构和数据流
- 简单前端界面（用于测试和演示）
- 协调团队进度

#### 第二天（核心功能）

**成员A：**
- 优化检索策略（相似度阈值、重排序）
- 构建问题分类器
- A/B 测试不同检索方案

**成员B：**
- Prompt 工程优化
- 实现上下文管理
- 多轮对话支持

**成员C：**
- 业务逻辑整合
- 异常处理和降级策略
- 初步评估测试

#### 第三天（优化和评估）

**全员协作：**
- 在测试集上批量运行
- 分析低分 case，针对性优化
- Prompt 迭代和参数调优
- 准备演示材料和技术文档

---

### 三、关键技术路径

```
用户问题 → 问题理解 → 向量检索 → 上下文构建 → LLM 生成 → 后处理 → 返回答案
```

**核心优化点：**
1. **检索质量**：高质量向量化 + 混合检索（向量+关键词）
2. **Prompt 设计**：提供历史最佳回答作为示例
3. **答案质量控制**：置信度评分 + 人工审核机制

---

### 四、风险控制

**高优先级：**
- Day 1 结束前必须跑通基础流程
- 每日设置里程碑验证点
- 准备降级方案（纯检索匹配）

**时间分配原则：**
- 60% 时间核心功能
- 30% 时间优化迭代
- 10% 时间演示准备

---

### 五、评分策略

**针对评分规则优化：**
- 先保证答案正确性（避免1-4分）
- 再追求答案质量（争取6-9分）
- 重点优化高频问题类型
- 对不确定的问题，返回保守但准确的答案

---

### 建议行动

**立即开始：**
1. 选定技术栈（建议 Dify + GPT-4 API）
2. 申请 API 密钥
3. 数据预处理启动

**成功关键：** 快速验证可行性 > 完美实现细节

---

以上方案充分考虑了三天时间限制和三人团队配置，重点是**快速集成成熟组件**而非从零开发。
